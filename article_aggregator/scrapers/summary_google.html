<!DOCTYPE html><html><body><p><meta name="twitter:image" content="https://4.bp.blogspot.com/-O2Yl1GRPVcE/WmvSPjUljRI/AAAAAAAAEbI/ubgviNZ6OQA9XfS2aIxtMcs8DPyitExewCLcBGAs/s1600/image2.png"> <em>Posted by Laurence Moroney, Developer Advocate</em> <p>We're delighted to announce that TensorFlow 1.5 is now public! Install it now to get a bunch of new features that we hope you'll enjoy! </p><h3>Eager Execution for TensorFlow</h3>  <p>First off, <a href="https://developers.googleblog.com/2017/10/eager-execution-imperative-define-by.html">Eager Execution for TensorFlow</a> is now available as a preview. We've heard lots of feedback about the programming style of TensorFlow, and how developers <em>really</em> want an imperative, define-by-run programming style. With Eager Execution for TensorFlow enabled, you can execute TensorFlow operations <em>immediately </em>as they are called from Python<em>. </em>This makes it easier to get started with TensorFlow, and can make research and development more intuitive. </p><p>For example, think of a simple computation like a matrix multiplication. Today, in TensorFlow it would look something like this: </p>   <pre class="prettyprint">x = tf.placeholder(tf.float32, shape=[1, 1])<br />m = tf.matmul(x, x)<br /><br />with tf.Session() as sess:<br />  print(sess.run(m, feed_dict={x: [[2.]]}))<br /></pre>  <p>If you enable Eager Execution for TensorFlow, it will look more like this: </p>   <pre class="prettyprint">x = [[2.]]<br />m = tf.matmul(x, x)<br /><br />print(m)<br /></pre>  <p>You can learn more about Eager Execution for TensorFlow <a href="https://github.com/tensorflow/tensorflow/tree/r1.5/tensorflow/contrib/eager">here</a> (check out the <a href="https://github.com/tensorflow/tensorflow/blob/r1.5/tensorflow/contrib/eager/python/g3doc/guide.md">user guide</a> linked at the bottom of the page, and also this <a href="http://goo.gl/74t66Q">presentation</a>) and the API docs <a href="https://www.tensorflow.org/versions/r1.5/api_docs/python/tf/contrib/eager">here</a>. </p>  <h3>TensorFlow Lite</h3>    <a href="https://3.bp.blogspot.com/-XHT8EUKBdh8/WmuizTDMp7I/AAAAAAAAEaM/crW9imdsNXQFy8uDvEO7-vDCgddm9lnfgCLcBGAs/s1600/image1.png" imageanchor="1" ><img border="0" src="https://3.bp.blogspot.com/-XHT8EUKBdh8/WmuizTDMp7I/AAAAAAAAEaM/crW9imdsNXQFy8uDvEO7-vDCgddm9lnfgCLcBGAs/s1600/image1.png" data-original-width="900" data-original-height="1600" style="float:right; max-width:30%;" /></a>  <p>The Developer preview of <a href="https://github.com/tensorflow/tensorflow/tree/r1.5/tensorflow/contrib/lite">TensorFlow Lite</a> is built into version 1.5. TensorFlow Lite, TensorFlow's lightweight solution for mobile and embedded devices, lets you take a trained TensorFlow model and convert it into a .tflite file which can then be executed on a mobile device with low-latency. Thus the <em>training</em> doesn't have to be done on the device, nor does the device need to upload data to the cloud to have it worked upon. So, for example, if you want to classify an image, a trained model could be deployed to the device and classification of the image is done on-device directly.  </p><p>TensorFlow Lite includes a <a href="https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2-tflite/index.html#1">sample app to get you started</a>. This app uses the MobileNet model of 1001 unique image categories. It recognizes an image and matches it to a number of categories, listing the top 3 that it recognizes. The app is available on both Android and iOS. </p><p>You can learn more about TensorFlow Lite, and how to convert your models to be available on mobile <a href="https://www.tensorflow.org/versions/r1.5/mobile/tflite/">here</a>. </p><h3>GPU Acceleration Updates</h3>  <p>If you are using GPU Acceleration on Windows or Linux, TensorFlow 1.5 now has CUDA 9 and cuDNN 7 support built-in.  </p><p>To learn more about NVidia's Compute Unified Device Architecture (CUDA) 9, check out NVidia's site <a href="https://developer.nvidia.com/cuda-toolkit/whatsnew">here</a>.  </p><p>This is enhanced by the <a href="https://developer.nvidia.com/cudnn">CUDA Deep Neural Network Library (cuDNN),</a> the latest release of which is version 7. Support for this is now included in TensorFlow 1.5. </p><p>Here are some Medium articles on GPU support on <a href="https://buzzrobot.com/installing-tensorflow-with-gpu-on-windows-10-3309fec55a00">Windows</a> and <a href="https://chatbotsmagazine.com/tensorflow-install-for-gpu-on-linux-902b404b6b30">Linux</a>, and how to install them on your workstation (if it supports the requisite hardware) </p><h3>Documentation Site Updates</h3>  <p>In line with this release we've also overhauled the documentation site, including an improved Getting Started flow that will get you from no knowledge to building a neural network to classify different types of iris in a very short time. <a href="https://www.tensorflow.org/versions/master/get_started/get_started_for_beginners">Check it out! </a></p> <a href="https://1.bp.blogspot.com/-yXzMMNtqXD4/Wmujp0W0V9I/AAAAAAAAEaU/0YQS02e6pk42ZlpL4Z65GwKQ4_FFIHkkACLcBGAs/s1600/image2.png" imageanchor="1" ><img border="0" src="https://1.bp.blogspot.com/-yXzMMNtqXD4/Wmujp0W0V9I/AAAAAAAAEaU/0YQS02e6pk42ZlpL4Z65GwKQ4_FFIHkkACLcBGAs/s1600/image2.png" data-original-width="1600" data-original-height="489" /></a> <h3>Other Enhancements</h3>  <p>Beyond these features, there's lots of other enhancements to Accelerated Linear Algebra (XLA), updates to RunConfig and much more. Check the <a href="https://github.com/tensorflow/tensorflow/blob/r1.5/RELEASE.md">release notes here.</a></p><h3>Installing TensorFlow 1.5</h3>  <p>To get TensorFlow 1.5, you can use the standard pip installation (or pip3 if you use python3) </p>   <pre class="prettyprint">$  pip install --ignore-installed --upgrade tensorflow</pre> <style> pre.prettyprint { padding: 10px; }  </style><img src="http://feeds.feedburner.com/~r/GDBcode/~4/Mri5dCS_ISI" height="1" width="1" alt=""/></p></body></html>